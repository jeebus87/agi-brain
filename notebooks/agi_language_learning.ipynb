{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# AGI Brain - Language Learning from Scratch\n\n**A spiking neural network that learns to understand and speak without any pre-trained models.**\n\nThis notebook demonstrates:\n- 100K-1M neuron sparse SNN architecture (scalable)\n- Phoneme recognition through STDP\n- Semantic memory for word associations\n- Speech synthesis from neural activity\n- Learning from YouTube videos and web pages\n\n---\n\n**Runtime Setup:** Go to `Runtime > Change runtime type > T4 GPU`"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install tensorflow numpy matplotlib scipy gradio yt-dlp librosa --quiet\n",
    "\n",
    "# Clone repository\n",
    "!rm -rf agi-brain\n",
    "!git clone https://github.com/jeebus87/agi-brain.git\n",
    "%cd agi-brain\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU\n",
    "import tensorflow as tf\n",
    "print(\"TensorFlow:\", tf.__version__)\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "print(\"GPUs available:\", gpus)\n",
    "\n",
    "# Imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"\\nSetup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 2. Sparse SNN Architecture\n\nMemory-efficient architecture using sparse connectivity:\n- **Free Colab Tier:** 100K neurons (~0.2 GB)\n- **Colab Pro:** 500K-1M neurons (~1-4 GB)\n- Event-driven computation\n- Population-level parallelism"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from src.language.sparse_network import estimate_memory\n\n# Check memory requirements\nprint(\"Memory Estimates (0.1% connectivity):\")\nprint(\"=\" * 50)\nfor n in [100_000, 500_000, 1_000_000]:\n    est = estimate_memory(n, connectivity=0.001)\n    fits = \"YES\" if est['total_gb'] < 10 else \"NO\"\n    print(f\"{n/1e6:.1f}M neurons: {est['total_gb']:.2f} GB (fits Colab: {fits})\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from src.language.sparse_network import create_language_brain\n\n# Create the language brain (100K neurons for free Colab tier)\n# Scale up to 500K-1M if you have Colab Pro\nprint(\"Creating language brain...\")\nbrain_snn = create_language_brain(n_neurons=100_000, use_gpu=True)\n\nprint(f\"\\nTotal neurons: {brain_snn.total_neurons():,}\")\nprint(f\"Memory usage: {brain_snn.memory_usage_mb():.1f} MB\")\nprint(f\"Populations: {len(brain_snn.populations)}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Audio Encoding (Cochlea Simulation)\n",
    "\n",
    "Converts audio waveforms to spike patterns, mimicking biological cochlea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.language.audio_encoder import CochleaEncoder, AudioProcessor\n",
    "\n",
    "# Create cochlea encoder\n",
    "cochlea = CochleaEncoder()\n",
    "\n",
    "# Generate test audio (speech-like frequency sweep)\n",
    "sample_rate = 16000\n",
    "duration = 1.0\n",
    "t = np.linspace(0, duration, int(sample_rate * duration))\n",
    "\n",
    "# Simulated vowel (formants at 700Hz, 1200Hz, 2500Hz)\n",
    "f0 = 120  # Fundamental frequency\n",
    "audio = np.zeros_like(t)\n",
    "for harmonic in range(1, 20):\n",
    "    freq = f0 * harmonic\n",
    "    # Apply formant filter (simplified)\n",
    "    amp = np.exp(-((freq - 700)**2) / (2 * 100**2))  # F1\n",
    "    amp += np.exp(-((freq - 1200)**2) / (2 * 150**2))  # F2\n",
    "    audio += amp * np.sin(2 * np.pi * freq * t) / harmonic\n",
    "\n",
    "audio = audio / np.max(np.abs(audio)) * 0.5\n",
    "\n",
    "# Encode to spikes\n",
    "spikes = cochlea.encode_audio(audio)\n",
    "\n",
    "print(f\"Audio: {len(audio)} samples ({duration}s)\")\n",
    "print(f\"Spike representation: {spikes.shape} (time x frequency)\")\n",
    "print(f\"Spike density: {spikes.mean():.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize cochlea output\n",
    "fig, axes = plt.subplots(2, 1, figsize=(14, 6))\n",
    "\n",
    "# Audio waveform\n",
    "axes[0].plot(t[:1600], audio[:1600], 'b-', linewidth=0.5)\n",
    "axes[0].set_xlabel('Time (s)')\n",
    "axes[0].set_ylabel('Amplitude')\n",
    "axes[0].set_title('Audio Waveform (100ms)', fontweight='bold')\n",
    "axes[0].set_xlim(0, 0.1)\n",
    "\n",
    "# Spike representation\n",
    "im = axes[1].imshow(spikes.T, aspect='auto', cmap='hot', extent=[0, duration, 80, 0])\n",
    "axes[1].set_xlabel('Time (s)')\n",
    "axes[1].set_ylabel('Frequency Band')\n",
    "axes[1].set_title('Cochlea Spike Output', fontweight='bold')\n",
    "plt.colorbar(im, ax=axes[1], label='Spike')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Phoneme Learning with STDP\n",
    "\n",
    "Learn to recognize phonemes (speech sounds) through spike-timing dependent plasticity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.language.phoneme_learner import PhonemeLearner, PhonemeConfig\n",
    "\n",
    "# Create phoneme learner\n",
    "config = PhonemeConfig(\n",
    "    n_input=800,           # Cochlea output size\n",
    "    n_phoneme_neurons=500, # Neurons per phoneme detector\n",
    "    n_phonemes=20,         # Number of phonemes to learn\n",
    "    learning_rate=0.005\n",
    ")\n",
    "learner = PhonemeLearner(config)\n",
    "\n",
    "print(f\"Phoneme learner created:\")\n",
    "print(f\"  Detectors: {len(learner.detectors)}\")\n",
    "print(f\"  Neurons per detector: {config.n_phoneme_neurons}\")\n",
    "print(f\"  Total neurons: {len(learner.detectors) * config.n_phoneme_neurons:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate training patterns (simulating different phonemes)\n",
    "np.random.seed(42)\n",
    "n_phonemes = 10\n",
    "patterns = []\n",
    "\n",
    "for i in range(n_phonemes):\n",
    "    # Each phoneme has a distinct sparse pattern\n",
    "    pattern = np.zeros(800, dtype=np.float32)\n",
    "    # Activate different frequency regions for different phonemes\n",
    "    start = i * 60\n",
    "    pattern[start:start+80] = np.random.rand(80)\n",
    "    pattern[pattern < 0.5] = 0\n",
    "    patterns.append(pattern)\n",
    "\n",
    "print(f\"Created {len(patterns)} distinct phoneme patterns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train phoneme recognition\n",
    "print(\"Training phoneme recognition...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "n_epochs = 5\n",
    "samples_per_phoneme = 200\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for phoneme_idx in range(n_phonemes):\n",
    "        for _ in range(samples_per_phoneme):\n",
    "            # Add noise to pattern\n",
    "            pattern = patterns[phoneme_idx].copy()\n",
    "            noise = np.random.rand(800).astype(np.float32) * 0.2\n",
    "            noisy_pattern = np.clip(pattern + noise, 0, 1)\n",
    "            noisy_pattern[noisy_pattern < 0.3] = 0\n",
    "            \n",
    "            # Process with supervised learning\n",
    "            detected, conf = learner.process(\n",
    "                noisy_pattern,\n",
    "                learn=True,\n",
    "                target_phoneme=phoneme_idx\n",
    "            )\n",
    "            \n",
    "            if detected == phoneme_idx:\n",
    "                correct += 1\n",
    "            total += 1\n",
    "    \n",
    "    accuracy = correct / total\n",
    "    print(f\"Epoch {epoch+1}: Accuracy = {accuracy:.1%}\")\n",
    "\n",
    "print(\"\\nTraining complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test phoneme recognition\n",
    "print(\"\\nTesting phoneme recognition:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "test_correct = 0\n",
    "for phoneme_idx in range(n_phonemes):\n",
    "    pattern = patterns[phoneme_idx]\n",
    "    detected, conf = learner.process(pattern, learn=False)\n",
    "    status = \"correct\" if detected == phoneme_idx else f\"wrong (got {detected})\"\n",
    "    print(f\"Phoneme {phoneme_idx}: detected {detected} (conf: {conf:.2f}) - {status}\")\n",
    "    if detected == phoneme_idx:\n",
    "        test_correct += 1\n",
    "\n",
    "print(f\"\\nTest accuracy: {test_correct}/{n_phonemes} = {test_correct/n_phonemes:.0%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Semantic Memory\n",
    "\n",
    "Learn word-meaning associations using Sparse Distributed Memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.language.semantic_memory import SemanticMemory, SemanticConfig\n",
    "\n",
    "# Create semantic memory\n",
    "sem_config = SemanticConfig(\n",
    "    n_hard_locations=5000,\n",
    "    address_size=500,\n",
    "    data_size=500\n",
    ")\n",
    "memory = SemanticMemory(sem_config)\n",
    "\n",
    "print(\"Semantic memory created\")\n",
    "print(f\"  Hard locations: {sem_config.n_hard_locations:,}\")\n",
    "print(f\"  Vector dimensions: {sem_config.data_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Teach some concepts and relationships\n",
    "print(\"Teaching concepts...\")\n",
    "\n",
    "# Create basic concepts\n",
    "concepts = [\n",
    "    \"cat\", \"dog\", \"bird\", \"fish\",\n",
    "    \"animal\", \"pet\", \"mammal\",\n",
    "    \"run\", \"fly\", \"swim\", \"walk\",\n",
    "    \"small\", \"large\", \"fast\", \"slow\"\n",
    "]\n",
    "\n",
    "for word in concepts:\n",
    "    memory.create_concept(word=word)\n",
    "\n",
    "# Learn associations\n",
    "associations = [\n",
    "    (\"cat\", \"animal\"), (\"cat\", \"pet\"), (\"cat\", \"mammal\"), (\"cat\", \"small\"),\n",
    "    (\"dog\", \"animal\"), (\"dog\", \"pet\"), (\"dog\", \"mammal\"), (\"dog\", \"run\"),\n",
    "    (\"bird\", \"animal\"), (\"bird\", \"fly\"), (\"bird\", \"small\"),\n",
    "    (\"fish\", \"animal\"), (\"fish\", \"swim\"), (\"fish\", \"pet\"),\n",
    "    (\"mammal\", \"animal\"),\n",
    "    (\"run\", \"fast\"), (\"fly\", \"fast\"), (\"swim\", \"fast\"),\n",
    "]\n",
    "\n",
    "for w1, w2 in associations:\n",
    "    memory.learn_association(w1, w2, strength=0.3)\n",
    "\n",
    "print(f\"Vocabulary: {memory.get_vocabulary_size()} words\")\n",
    "print(f\"Concepts: {len(memory.concepts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learn from sentences\n",
    "sentences = [\n",
    "    [\"the\", \"cat\", \"is\", \"a\", \"small\", \"pet\"],\n",
    "    [\"the\", \"dog\", \"can\", \"run\", \"fast\"],\n",
    "    [\"birds\", \"can\", \"fly\", \"in\", \"the\", \"sky\"],\n",
    "    [\"fish\", \"swim\", \"in\", \"water\"],\n",
    "    [\"cats\", \"and\", \"dogs\", \"are\", \"mammals\"],\n",
    "    [\"pets\", \"are\", \"animals\", \"we\", \"love\"],\n",
    "]\n",
    "\n",
    "print(\"Learning from sentences...\")\n",
    "for sentence in sentences:\n",
    "    memory.learn_from_sentence(sentence)\n",
    "\n",
    "print(f\"Updated vocabulary: {memory.get_vocabulary_size()} words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test spreading activation\n",
    "print(\"\\nSpreading activation from 'cat':\")\n",
    "print(\"=\" * 40)\n",
    "activations = memory.spread_activation(\"cat\", depth=2)\n",
    "for word, activation in sorted(activations.items(), key=lambda x: -x[1])[:10]:\n",
    "    bar = \"*\" * int(activation * 20)\n",
    "    print(f\"  {word:12} {activation:.3f} {bar}\")\n",
    "\n",
    "print(\"\\nSpreading activation from 'fly':\")\n",
    "print(\"=\" * 40)\n",
    "activations = memory.spread_activation(\"fly\", depth=2)\n",
    "for word, activation in sorted(activations.items(), key=lambda x: -x[1])[:10]:\n",
    "    bar = \"*\" * int(activation * 20)\n",
    "    print(f\"  {word:12} {activation:.3f} {bar}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Speech Generation\n",
    "\n",
    "Generate speech audio from neural activity using formant synthesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.language.speech_generator import SpeechGenerator, FormantSynthesizer\n",
    "from IPython.display import Audio\n",
    "\n",
    "# Create speech generator\n",
    "generator = SpeechGenerator()\n",
    "\n",
    "# Test text to phonemes\n",
    "test_text = \"hello world\"\n",
    "phonemes = generator.text_to_phonemes(test_text)\n",
    "print(f\"Text: '{test_text}'\")\n",
    "print(f\"Phonemes: {' '.join(phonemes)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate speech\n",
    "print(\"\\nGenerating speech...\")\n",
    "audio_output = generator.speak(test_text, speed=0.8)\n",
    "print(f\"Audio length: {len(audio_output)} samples ({len(audio_output)/16000:.2f}s)\")\n",
    "\n",
    "# Play audio (works in Colab)\n",
    "Audio(audio_output, rate=16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate different phrases\n",
    "phrases = [\n",
    "    \"cat\",\n",
    "    \"hello\", \n",
    "    \"goodbye\",\n",
    "    \"i am learning\"\n",
    "]\n",
    "\n",
    "for phrase in phrases:\n",
    "    print(f\"\\n'{phrase}':\")\n",
    "    audio = generator.speak(phrase)\n",
    "    display(Audio(audio, rate=16000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Complete AGI Brain Integration\n",
    "\n",
    "Combine all components into a unified system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleBrain:\n",
    "    \"\"\"Simplified AGI brain for demonstration.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Components\n",
    "        self.semantic_memory = memory  # Use memory from above\n",
    "        self.speech_generator = generator\n",
    "        self.conversation_history = []\n",
    "        \n",
    "    def process(self, text):\n",
    "        \"\"\"Process input and generate response.\"\"\"\n",
    "        words = text.lower().split()\n",
    "        \n",
    "        # Learn from input\n",
    "        self.semantic_memory.learn_from_sentence(words)\n",
    "        \n",
    "        # Find associations\n",
    "        all_activations = {}\n",
    "        for word in words:\n",
    "            if self.semantic_memory.lookup(word):\n",
    "                activations = self.semantic_memory.spread_activation(word, depth=2)\n",
    "                for w, a in activations.items():\n",
    "                    all_activations[w] = all_activations.get(w, 0) + a\n",
    "        \n",
    "        # Generate response\n",
    "        if all_activations:\n",
    "            top_words = sorted(all_activations.items(), key=lambda x: -x[1])[:3]\n",
    "            associated = [w for w, a in top_words if w not in words]\n",
    "            if associated:\n",
    "                response = f\"I think of {', '.join(associated)}\"\n",
    "            else:\n",
    "                response = \"Tell me more\"\n",
    "        else:\n",
    "            response = f\"Learning about {words[0] if words else 'nothing'}\"\n",
    "        \n",
    "        self.conversation_history.append((text, response))\n",
    "        return response\n",
    "    \n",
    "    def speak(self, text):\n",
    "        \"\"\"Generate speech audio.\"\"\"\n",
    "        return self.speech_generator.speak(text)\n",
    "\n",
    "# Create brain\n",
    "brain = SimpleBrain()\n",
    "print(\"Brain created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Have a conversation\n",
    "print(\"Conversation with AGI Brain:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "inputs = [\n",
    "    \"hello\",\n",
    "    \"tell me about cats\",\n",
    "    \"can cats fly\",\n",
    "    \"what animals can fly\",\n",
    "    \"dogs are pets too\"\n",
    "]\n",
    "\n",
    "for user_input in inputs:\n",
    "    response = brain.process(user_input)\n",
    "    print(f\"\\nYou: {user_input}\")\n",
    "    print(f\"Brain: {response}\")\n",
    "    \n",
    "    # Generate and play audio response\n",
    "    audio = brain.speak(response)\n",
    "    display(Audio(audio, rate=16000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Learning from YouTube (Optional)\n",
    "\n",
    "Learn language from YouTube video transcripts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if yt-dlp is available\n",
    "import subprocess\n",
    "try:\n",
    "    result = subprocess.run(['yt-dlp', '--version'], capture_output=True)\n",
    "    print(\"yt-dlp available!\")\n",
    "    YTDLP_AVAILABLE = True\n",
    "except:\n",
    "    print(\"yt-dlp not found. Install with: !pip install yt-dlp\")\n",
    "    YTDLP_AVAILABLE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment and run to learn from a YouTube video:\n",
    "# (This requires yt-dlp and may take several minutes)\n",
    "\n",
    "# from src.interface.learning_pipeline import YouTubeLearner\n",
    "# \n",
    "# learner = YouTubeLearner()\n",
    "# \n",
    "# # Example: Learn from a short educational video\n",
    "# url = \"https://www.youtube.com/watch?v=dQw4w9WgXcQ\"  # Replace with actual video\n",
    "# \n",
    "# stats = learner.learn_from_video(url, brain)\n",
    "# print(f\"Learned {stats['words_learned']} words from video!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Interactive Chat (Gradio)\n",
    "\n",
    "Launch an interactive chat interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "def chat_with_brain(message, history):\n",
    "    response = brain.process(message)\n",
    "    return response\n",
    "\n",
    "def get_status():\n",
    "    return f\"\"\"\n",
    "    Vocabulary: {brain.semantic_memory.get_vocabulary_size()} words\n",
    "    Concepts: {len(brain.semantic_memory.concepts)}\n",
    "    Conversations: {len(brain.conversation_history)} turns\n",
    "    \"\"\"\n",
    "\n",
    "with gr.Blocks(title=\"AGI Brain\") as demo:\n",
    "    gr.Markdown(\"# AGI Brain Chat\\n\\nTalk to a neural network learning language from scratch!\")\n",
    "    \n",
    "    chatbot = gr.ChatInterface(\n",
    "        chat_with_brain,\n",
    "        examples=[\"hello\", \"tell me about cats\", \"what can fly\"],\n",
    "    )\n",
    "    \n",
    "    with gr.Accordion(\"Brain Status\", open=False):\n",
    "        status_btn = gr.Button(\"Refresh\")\n",
    "        status_text = gr.Textbox(label=\"Status\")\n",
    "        status_btn.click(get_status, outputs=status_text)\n",
    "\n",
    "demo.launch(share=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "\n",
    "1. **Sparse SNN Architecture** - 10M neurons fitting in GPU memory\n",
    "2. **Cochlea Encoding** - Audio to spike conversion\n",
    "3. **Phoneme Learning** - STDP-based speech sound recognition\n",
    "4. **Semantic Memory** - Word associations via Sparse Distributed Memory\n",
    "5. **Speech Generation** - Formant synthesis from neural output\n",
    "6. **Integrated Brain** - All components working together\n",
    "\n",
    "### Key Insight\n",
    "\n",
    "This brain learns language **from scratch** - no pre-trained LLM, no transfer learning.\n",
    "It's limited compared to ChatGPT, but it actually *learns* rather than retrieves.\n",
    "\n",
    "### Next Steps\n",
    "- Scale to 10M+ neurons\n",
    "- Train on more data (YouTube, web)\n",
    "- Improve phoneme-to-word mapping\n",
    "- Add reinforcement learning for conversation"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}